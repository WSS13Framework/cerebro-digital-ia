import streamlit as st
import os
import tempfile
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="C√©rebro Digital de IA - Demonstra√ß√£o REAL",
    page_icon="üß†",
    layout="wide"
)

# --- Header ---
st.title("üß† C√©rebro Digital de IA: Demonstra√ß√£o com SEUS Documentos")
st.markdown("""
### üéØ **Teste AGORA com seus pr√≥prios arquivos!**
Fa√ßa upload de seus documentos e veja a IA responder baseada **exclusivamente** no seu conte√∫do.
""")

# --- Configura√ß√£o API ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("üîë ERRO: Chave da API OpenAI n√£o configurada.")
    st.info("Entre em contato com o administrador para configurar a demonstra√ß√£o.")
    st.stop()

# --- Fun√ß√µes ---
def load_document(uploaded_file):
    """Carrega diferentes tipos de documentos"""
    suffix = os.path.splitext(uploaded_file.name)[1]
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    try:
        if suffix == ".pdf":
            loader = PyPDFLoader(tmp_path)
        elif suffix == ".docx":
            loader = Docx2txtLoader(tmp_path)
        elif suffix == ".txt":
            loader = TextLoader(tmp_path, encoding='utf-8')
        else:
            return None
        
        documents = loader.load()
        os.unlink(tmp_path)
        return documents
    except Exception as e:
        st.error(f"Erro ao carregar arquivo: {e}")
        return None

def setup_rag_system(documents):
    """Configura o sistema RAG com os documentos do cliente"""
    try:
        # Dividir texto
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)
        
        # Criar embeddings e vector store
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = Chroma.from_documents(texts, embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        
        # Criar prompt
        template = """Voc√™ √© um assistente especializado em responder perguntas sobre os documentos fornecidos.

REGRAS IMPORTANTES:
1. Use APENAS as informa√ß√µes do contexto abaixo
2. Se a informa√ß√£o N√ÉO estiver no contexto, diga claramente: "N√£o encontrei essa informa√ß√£o nos documentos fornecidos"
3. Seja preciso e cite trechos relevantes quando poss√≠vel
4. Responda em portugu√™s de forma clara e profissional

Contexto dos documentos:
{context}

Pergunta do usu√°rio: {question}

Resposta:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # Configurar LLM
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            openai_api_key=OPENAI_API_KEY
        )
        
        # Criar chain
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        return rag_chain, retriever, len(texts)
    
    except Exception as e:
        st.error(f"Erro ao configurar sistema: {e}")
        return None, None, 0

# --- Sidebar: Upload ---
with st.sidebar:
    st.header("üìÅ Fa√ßa Upload dos Seus Documentos")
    st.markdown("**Formatos aceitos:** PDF, DOCX, TXT")
    
    uploaded_files = st.file_uploader(
        "Arraste seus arquivos aqui",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True,
        help="Voc√™ pode enviar m√∫ltiplos arquivos"
    )
    
    st.markdown("---")
    st.markdown("""
    ### üí° Como funciona:
    1. **Upload**: Envie seus documentos
    2. **Processamento**: IA l√™ e indexa
    3. **Teste**: Fa√ßa perguntas
    4. **Valida√ß√£o**: Veja as fontes
    
    ### ‚úÖ Benef√≠cios:
    - Respostas baseadas 100% nos seus docs
    - Sem "alucina√ß√µes" de IA
    - Rastreabilidade total
    - Privacidade garantida
    """)

# --- Main: Processamento ---
if uploaded_files:
    with st.spinner("üîÑ Processando seus documentos..."):
        all_documents = []
        
        for uploaded_file in uploaded_files:
            docs = load_document(uploaded_file)
            if docs:
                all_documents.extend(docs)
                st.sidebar.success(f"‚úÖ {uploaded_file.name}")
        
        if all_documents:
            rag_chain, retriever, num_chunks = setup_rag_system(all_documents)
            
            if rag_chain and retriever:
                st.success(f"""
                ‚úÖ **Sistema pronto!**
                - üìÑ {len(uploaded_files)} arquivo(s) processado(s)
                - üß© {num_chunks} fragmentos indexados
                - üöÄ Pronto para responder suas perguntas!
                """)
                
                st.markdown("---")
                
                # --- Chat Interface ---
                st.subheader("üí¨ Fa√ßa suas perguntas sobre os documentos")
                
                if "messages" not in st.session_state:
                    st.session_state.messages = []
                
                # Mostrar hist√≥rico
                for message in st.session_state.messages:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])
                        if "sources" in message:
                            with st.expander("üìÑ Fontes consultadas"):
                                for i, source in enumerate(message["sources"], 1):
                                    st.code(f"Trecho {i}:\n{source}", language="text")
                
                # Input do usu√°rio
                if prompt := st.chat_input("Digite sua pergunta sobre os documentos..."):
                    # Adicionar pergunta
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"):
                        st.markdown(prompt)
                    
                    # Processar resposta
                    with st.spinner("ü§î Analisando documentos..."):
                        try:
                            # Buscar documentos relevantes
                            source_docs = retriever.get_relevant_documents(prompt)
                            
                            # Gerar resposta
                            response = rag_chain.invoke(prompt)
                            
                            # Preparar fontes
                            sources = [doc.page_content[:300] + "..." for doc in source_docs]
                            
                            # Adicionar resposta
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": response,
                                "sources": sources
                            })
                            
                            with st.chat_message("assistant"):
                                st.markdown(response)
                                with st.expander("üìÑ Fontes consultadas"):
                                    for i, source in enumerate(sources, 1):
                                        st.code(f"Trecho {i}:\n{source}", language="text")
                        
                        except Exception as e:
                            st.error(f"‚ùå Erro ao processar: {e}")
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": "Desculpe, ocorreu um erro ao processar sua pergunta."
                            })

else:
    # --- Tela inicial sem upload ---
    st.info("üëÜ **Comece fazendo upload de seus documentos na barra lateral**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üéØ Por que testar com SEUS documentos?
        
        ‚úÖ **Prova real do conceito**  
        N√£o acredite s√≥ em demos - teste com seu conte√∫do!
        
        ‚úÖ **Valida√ß√£o imediata**  
        Veja se a IA entende SEU neg√≥cio
        
        ‚úÖ **Zero setup**  
        Sem instala√ß√£o, sem configura√ß√£o
        
        ‚úÖ **100% privado**  
        Seus documentos n√£o s√£o armazenados
        """)
    
    with col2:
        st.markdown("""
        ### üìä Casos de uso ideais:
        
        - üìö Manuais e documenta√ß√£o t√©cnica
        - üìã Pol√≠ticas e procedimentos internos
        - üìÑ Contratos e termos legais
        - üè¢ Relat√≥rios corporativos
        - üìñ Base de conhecimento de produtos
        - üíº Documentos de compliance
        """)
    
    st.markdown("---")
    st.warning("‚ö†Ô∏è **Demonstra√ß√£o:** Esta √© uma POC. Para produ√ß√£o, implementamos seguran√ßa adicional, controle de acesso e integra√ß√µes personalizadas.")

# --- Footer ---
st.markdown("---")
st.caption("üß† C√©rebro Digital de IA | Demonstra√ß√£o de Tecnologia RAG (Retrieval-Augmented Generation)")