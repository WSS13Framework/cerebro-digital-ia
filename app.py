import streamlit as st
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- Configura√ß√£o da P√°gina Streamlit ---
st.set_page_config(page_title="C√©rebro Digital de IA - Demonstra√ß√£o", layout="wide")
st.title("üß† C√©rebro Digital de IA: Seu Assistente de Conte√∫do Imediato")
st.subheader("Demonstra√ß√£o de Prova de Conceito (POC) para Clientes")

# --- Vari√°veis de Configura√ß√£o ---
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    st.error("üîë ERRO: A chave da API da OpenAI n√£o est√° configurada.")
    st.info("""
    **Para configurar no Streamlit Cloud:**
    1. V√° em Settings ‚Üí Secrets
    2. Adicione: `OPENAI_API_KEY = "sua-chave-aqui"`
    
    **Para rodar localmente:**
    Execute: `export OPENAI_API_KEY='sua-chave-aqui'`
    """)
    st.stop()

# --- Fun√ß√µes do RAG ---

@st.cache_resource
def setup_rag_system(file_path):
    """
    Configura o sistema RAG (Retrieval-Augmented Generation) moderno.
    """
    try:
        # 1. Carregar o documento
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()

        # 2. Dividir o texto
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        texts = text_splitter.split_documents(documents)

        # 3. Criar Embeddings e Vector Store
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = Chroma.from_documents(texts, embeddings)
        retriever = vectorstore.as_retriever()
        
        # 4. Criar o prompt template
        template = """Voc√™ √© um assistente especializado em responder perguntas sobre pol√≠ticas da empresa.
Use APENAS as informa√ß√µes do contexto abaixo para responder. Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o sabe.

Contexto:
{context}

Pergunta: {question}

Resposta:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # 5. Configurar o LLM
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.1, openai_api_key=OPENAI_API_KEY)
        
        # 6. Criar a chain moderna
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        return rag_chain, retriever

    except Exception as e:
        st.error(f"Erro ao configurar o sistema RAG: {e}")
        return None, None

# --- Inicializa√ß√£o ---
FILE_PATH = "politicas_empresa.txt"

if not os.path.exists(FILE_PATH):
    st.error(f"‚ùå Arquivo '{FILE_PATH}' n√£o encontrado!")
    st.info("Certifique-se de que o arquivo est√° no reposit√≥rio Git.")
    st.stop()

rag_chain, retriever = setup_rag_system(FILE_PATH)

if rag_chain and retriever:
    st.success(f"‚úÖ Sistema treinado com sucesso! (Base: {os.path.basename(FILE_PATH)})")
    st.markdown("---")

    # --- Interface de Chat ---
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Exibe o hist√≥rico de chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Captura a entrada do usu√°rio
    if prompt := st.chat_input("Pergunte algo sobre as pol√≠ticas da empresa..."):
        # Adiciona a mensagem do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Processa a pergunta
        with st.spinner("ü§î A IA est√° consultando a base de conhecimento..."):
            try:
                # Buscar documentos relevantes
                source_docs = retriever.get_relevant_documents(prompt)
                
                # Gerar resposta
                response = rag_chain.invoke(prompt)
                
                # Adiciona a resposta da IA ao hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response})
                with st.chat_message("assistant"):
                    st.markdown(response)
                    
                    # Mostra a fonte para o cliente
                    with st.expander("üìÑ Ver Fontes Consultadas"):
                        for i, doc in enumerate(source_docs, 1):
                            st.code(f"Fonte {i}:\n{doc.page_content[:200]}...", language="text")

            except Exception as e:
                st.error(f"‚ùå Ocorreu um erro durante a consulta: {e}")
                st.session_state.messages.append({"role": "assistant", "content": "Desculpe, ocorreu um erro ao processar sua solicita√ß√£o."})

# --- Instru√ß√µes para o Cliente ---
st.sidebar.title("üìã Instru√ß√µes para a Demonstra√ß√£o")
st.sidebar.markdown("""
Este prot√≥tipo demonstra como sua IA pode responder **apenas** com base nos seus documentos internos.

**Perguntas de Teste:**
1. Qual √© a pol√≠tica de home office da TechCorp?
2. Qual o prazo para submeter despesas de viagem?
3. Qual √© a miss√£o da empresa?
4. Quantos dias de f√©rias tenho direito?

**O que o cliente v√™:** 
- ‚úÖ Respostas precisas
- ‚úÖ Fonte da informa√ß√£o
- ‚úÖ Baseado 100% nos documentos

**O que o cliente compra:** 
A certeza de que a IA n√£o "alucina" e usa apenas o conhecimento da empresa.
""")

st.sidebar.markdown("---")
st.sidebar.info(f"üîë API Key: {'‚úÖ Configurada' if OPENAI_API_KEY else '‚ùå Ausente'}")
st.sidebar.info(f"üìÅ Arquivo: {FILE_PATH}")